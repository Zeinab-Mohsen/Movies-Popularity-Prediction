{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVR\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# One hot encoding\n",
    "genres_mlb = MultiLabelBinarizer()\n",
    "spoken_languages_mlb = MultiLabelBinarizer()\n",
    "production_countries_mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Label encoding\n",
    "original_language_encoder = LabelEncoder()\n",
    "status_encoder = LabelEncoder()\n",
    "name_encoder = LabelEncoder()\n",
    "character_encoder = LabelEncoder()\n",
    "\n",
    "# Hashing encoding\n",
    "keywords_hash_num = 30    #Change this Number for more accuracy\n",
    "keywords_hash_columns = []\n",
    "hash_column_name = ''\n",
    "\n",
    "for i in range(keywords_hash_num):\n",
    "  hash_column_name = 'keywords_hash_' + str(i)\n",
    "  keywords_hash_columns.append(hash_column_name)\n",
    "\n",
    "keywords_hasher = FeatureHasher(n_features=keywords_hash_num, input_type='string')\n",
    "\n",
    "\n",
    "production_companies_hash_num = 20 #Change this Number for more accuracy\n",
    "production_companies_hash_columns = []\n",
    "hash_column_name = ''\n",
    "\n",
    "for i in range(production_companies_hash_num):\n",
    "  hash_column_name = 'production_companies_hash_' + str(i)\n",
    "  production_companies_hash_columns.append(hash_column_name)\n",
    "\n",
    "production_companies_hasher = FeatureHasher(n_features=production_companies_hash_num, input_type='string')\n",
    "\n",
    "\n",
    "tfidf  = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "\n",
    "cast_num = 1  \n",
    "cast_columns = []\n",
    "cast_character = ''\n",
    "cast_gender = ''\n",
    "cast_name = ''\n",
    "cast_order = ''\n",
    "\n",
    "for i in range(cast_num):\n",
    "  cast_character = 'cast_' + str(i) + '_character'\n",
    "  cast_gender = 'cast_' + str(i) + '_gender'\n",
    "  cast_name = 'cast_' + str(i) + '_name'\n",
    "  cast_order = 'cast_' + str(i) + '_order'\n",
    "\n",
    "  cast_columns.append(cast_character)\n",
    "  cast_columns.append(cast_gender)\n",
    "  cast_columns.append(cast_name)\n",
    "  cast_columns.append(cast_order)\n",
    "\n",
    "\n",
    "missingValues = {\n",
    "    'budget'  :  None,\n",
    "    'genres'  :  None,\n",
    "    'homepage'  :  None,\n",
    "    'id'  :  None,\n",
    "    'keywords'  :  None,\n",
    "    'original_language'  :  None,\n",
    "    'original_title'  :  None,\n",
    "    'overview'  :  None,\n",
    "    'viewercount'  :  None,\n",
    "    'production_companies'  :  None,\n",
    "    'production_countries'  :  None,\n",
    "    'release_date'  :  None,\n",
    "    'revenue'  :  None,\n",
    "    'runtime'  :  None,\n",
    "    'spoken_languages'  :  None,\n",
    "    'status'  :  None,\n",
    "    'tagline'  :  None,\n",
    "    'title'  :  None,\n",
    "    'vote_count'  :  None,\n",
    "    'cast'  :  None,\n",
    "    'crew'  :  None,\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the encoding objects\n",
    "def load_encoders():\n",
    "    with open('encoders.pkl', 'rb') as file:\n",
    "        encodings = pickle.load(file)\n",
    "\n",
    "        \n",
    "    genres_mlb = encodings['genres_mlb']\n",
    "    spoken_languages_mlb = encodings['spoken_languages_mlb']\n",
    "    production_countries_mlb = encodings['production_countries_mlb']\n",
    "    original_language_encoder = encodings['original_language_encoder']\n",
    "    status_encoder = encodings['status_encoder']\n",
    "    name_encoder = encodings['name_encoder']\n",
    "    character_encoder = encodings['character_encoder']\n",
    "    \n",
    "    return genres_mlb, spoken_languages_mlb, production_countries_mlb, original_language_encoder, status_encoder, name_encoder, character_encoder\n",
    "\n",
    "# Load the feature hashers\n",
    "def load_hashers():\n",
    "    with open('hashers.pkl', 'rb') as file:\n",
    "        hashing = pickle.load(file)\n",
    "\n",
    "    keywords_hasher = hashing['keywords_hasher']\n",
    "    production_companies_hasher = hashing['production_companies_hasher']\n",
    "\n",
    "    return keywords_hasher, production_companies_hasher\n",
    "    \n",
    "\n",
    "# Load the TF-IDF vectorizer\n",
    "def load_tfidf_vectorizer():\n",
    "    with open('tfidf_vectorizer.pkl', 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "# Load the scalers\n",
    "def load_scalers():\n",
    "    with open('scalers.pkl', 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "# Load the feature selection\n",
    "def load_feature_selection():\n",
    "    with open('feature_selection.pkl', 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "# Load the models\n",
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "    \n",
    "# Load the missing values\n",
    "def load_missing_values():\n",
    "    with open('missing_values.pkl', 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a new label in label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unseen(data, encoder, columnName):\n",
    "    unseen_values = list(set(data[columnName]) - set(encoder.classes_))\n",
    "    if unseen_values:\n",
    "        for unseen in unseen_values:\n",
    "            if unseen not in encoder.classes_:\n",
    "                new_label = max(encoder.transform(encoder.classes_)) + 1\n",
    "                encoder.classes_ = np.append(encoder.classes_, unseen)\n",
    "                encoder.transform([unseen])[0] = new_label\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting list of dictionaries to normal list of elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformColumn(enteredData, columnName, dictionaryKey):\n",
    "    enteredData[columnName] = [ast.literal_eval(row) for row in enteredData[columnName]]\n",
    "    for index, row in enteredData[columnName].items():\n",
    "        finalList = []\n",
    "        for j in range(len(row)):\n",
    "            finalList.append(row[j][dictionaryKey])\n",
    "        enteredData.at[index, columnName] = finalList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformMoviesColumns(enteredData, columnName, dictionaryKey1, dictionaryKey2, dictionaryKey3, dictionaryKey4):\n",
    "  enteredData[columnName] = [ast.literal_eval(row) for row in enteredData[columnName]]\n",
    "  for index, row in enteredData[columnName].items():\n",
    "    finalList = []\n",
    "    for j in range(len(row)):\n",
    "      fList = []\n",
    "      fList.append(row[j][dictionaryKey1])\n",
    "      fList.append(row[j][dictionaryKey2])\n",
    "      fList.append(row[j][dictionaryKey3])\n",
    "      fList.append(row[j][dictionaryKey4])\n",
    "      finalList.append(fList)\n",
    "    enteredData.at[index, columnName] = finalList\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fillMissingTestData(data):\n",
    "    missingValues = load_missing_values()\n",
    "    \n",
    "    categoralColumns = ['genres', 'keywords', 'spoken_languages',\n",
    "                        'production_companies', 'production_countries', 'cast', 'crew']\n",
    "    for i in categoralColumns:\n",
    "        data[i] = data[i].apply(lambda x: x if x else missingValues[i])\n",
    "   \n",
    "    numericalColumns = ['budget', 'id', 'viewercount',\n",
    "                    'release_date', 'revenue', 'runtime', 'vote_count']\n",
    "    for i in numericalColumns:\n",
    "        data[i] = data[i].replace(0, missingValues[i])\n",
    "        data[i] =  data[i].fillna(missingValues[i])\n",
    "\n",
    "    textualColumns = ['homepage', 'original_title', 'tagline',\n",
    "                      'title', 'status', 'overview', 'original_language']\n",
    "    for i in textualColumns:\n",
    "        data[i] =  data[i].fillna(missingValues[i])\n",
    "\n",
    "    return data\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the cast and crew columns to X dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_columns(data, movies):\n",
    "  # Add two empty columns to the DataFrame\n",
    "  data = data.join(pd.DataFrame(movies['cast'], columns=['cast'], index = data.index))\n",
    "  data = data.join(pd.DataFrame(movies['crew'], columns=['crew'], index = data.index))\n",
    "\n",
    "  # Put the cast and crew in the right cells\n",
    "  for dataIndex, dataRow in data.iterrows():\n",
    "    if dataRow['id'] in movies['movie_id'].values:\n",
    "      index = movies.loc[movies['movie_id'] == dataRow['id']].index[0]\n",
    "      data.at[dataIndex, 'cast'] = movies.at[index, 'cast']\n",
    "      data.at[dataIndex, 'crew'] = movies.at[index, 'crew']\n",
    "    else:\n",
    "      data.at[dataIndex, 'cast'] = '[]'\n",
    "      data.at[dataIndex, 'crew'] = '[]'\n",
    "  \n",
    "  return data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nonModels_preprocessing_script(data):\n",
    "    # converting list of dectionaries to normal lists of elements using \"transformColumn\" function\n",
    "    # genres column\n",
    "    transformColumn(data, 'genres', 'name')\n",
    "    # keywords column\n",
    "    transformColumn(data, 'keywords', 'name')\n",
    "    # production_companies column\n",
    "    transformColumn(data, 'production_companies', 'name')\n",
    "    # production_countries column\n",
    "    transformColumn(data, 'production_countries', 'name')\n",
    "    # spoken_languages column\n",
    "    transformColumn(data, 'spoken_languages', 'iso_639_1')\n",
    "    # cast column\n",
    "    transformMoviesColumns(data, 'cast', 'character', 'gender', 'name', 'order')\n",
    "    # crew column\n",
    "    transformMoviesColumns(data, 'crew', 'name', 'department', 'gender', 'job')\n",
    "\n",
    "    # Only leaving the year from the release date column\n",
    "    data['release_date'] = data['release_date'].str[-4:].astype(int)\n",
    "\n",
    "    data = fillMissingTestData(data)\n",
    "\n",
    "    # Placing the values from the cast column to a column of it's own  \n",
    "    data = data.join(pd.DataFrame(columns=cast_columns, index = data.index))\n",
    "    for index, row in data.iterrows():\n",
    "      k = 0\n",
    "      for j in range(cast_num):\n",
    "        if(j > len(row['cast']) - 1):\n",
    "          break \n",
    "        data.at[index, cast_columns[k]] = row['cast'][j][0]\n",
    "        k += 1\n",
    "        data.at[index, cast_columns[k]] = row['cast'][j][1]\n",
    "        k += 1\n",
    "        data.at[index, cast_columns[k]] = row['cast'][j][2]\n",
    "        k += 1\n",
    "        data.at[index, cast_columns[k]] = row['cast'][j][3]\n",
    "        k += 1\n",
    "\n",
    "    data.drop('cast',axis=1,inplace=True)\n",
    "    \n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing test Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the loaded encodings to the test data\n",
    "def apply_encodings(data):\n",
    "    # Load the encoding objects, hashers, vectorizer, and scalers\n",
    "    genres_mlb, spoken_languages_mlb, production_countries_mlb, original_language_encoder, status_encoder, name_encoder, character_encoder =  load_encoders()\n",
    "    keywords_hasher, production_companies_hasher =  load_hashers()\n",
    "    tfidf = load_tfidf_vectorizer()\n",
    "    scaler = load_scalers()\n",
    "    \n",
    "    # Apply one-hot encoding\n",
    "    # genres column\n",
    "    data = data.join(pd.DataFrame(genres_mlb.transform(data.pop('genres')),\n",
    "                                    columns=genres_mlb.classes_,\n",
    "                                    index=data.index))\n",
    "    \n",
    "    # spoken_languages column\n",
    "    data = data.join(pd.DataFrame(spoken_languages_mlb.transform(data.pop('spoken_languages')),\n",
    "                                    columns=spoken_languages_mlb.classes_,\n",
    "                                    index=data.index))\n",
    "    # production_countries column \n",
    "    data = data.join(pd.DataFrame(production_countries_mlb.transform(data.pop('production_countries')),\n",
    "                                    columns=production_countries_mlb.classes_,\n",
    "                                    index=data.index))\n",
    "    \n",
    "\n",
    "    # Apply lable encoding\n",
    "    # original_language column\n",
    "    unseen(data, original_language_encoder, 'original_language')\n",
    "    data['original_language'] = original_language_encoder.transform(data['original_language'])\n",
    "    # status column\n",
    "    unseen(data, status_encoder, 'status')\n",
    "    data['status'] = status_encoder.transform(data['status'])\n",
    "    # cast column\n",
    "    for i in range(cast_num):\n",
    "        cast_name = 'cast_' + str(i) + '_name'\n",
    "        \n",
    "        unseen(data, name_encoder, cast_name)\n",
    "        data[cast_name] = name_encoder.transform(data[cast_name])\n",
    "\n",
    "\n",
    "        cast_character = 'cast_' + str(i) + '_character'\n",
    "\n",
    "        unseen(data, character_encoder, cast_character)\n",
    "        data[cast_character] = character_encoder.transform(data[cast_character])\n",
    "            \n",
    "    # Apply hashing encoding\n",
    "    # keywords column\n",
    "    data = data.join(pd.DataFrame((keywords_hasher.transform(data.pop('keywords')).toarray()), columns=keywords_hash_columns, index=data.index))\n",
    "    # production_companies column\n",
    "    data = data.join(pd.DataFrame((production_companies_hasher.transform(data.pop('production_companies')).toarray()), columns=production_companies_hash_columns, index=data.index))\n",
    "\n",
    "\n",
    "    # Apply TF-IDF \n",
    "    overview_vectors  = tfidf.transform(data['overview'])        \n",
    "    data['overview'] = list(overview_vectors.toarray())\n",
    "    data['overview'] = data['overview'].apply(lambda x: sum(x) / len(x))\n",
    "\n",
    "\n",
    "    # Normalizing the numerical columns\n",
    "    num_cols = data.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "    print(len(num_cols))\n",
    "    for column in num_cols:\n",
    "        print(f\"Column '{column}': {data[column].dtype}\")\n",
    "    data[num_cols] = scaler.transform(data[num_cols])\n",
    "\n",
    "\n",
    "        \n",
    "    data.drop('homepage',axis=1,inplace=True)\n",
    "    data.drop('id',axis=1,inplace=True)\n",
    "    data.drop('original_title',axis=1,inplace=True)\n",
    "    data.drop('tagline',axis=1,inplace=True)\n",
    "    data.drop('title',axis=1,inplace=True)\n",
    "    data.drop('crew',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zozom\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:861: UserWarning: unknown class(es) ['am', 'bs', 'ca', 'ce', 'co', 'km', 'mi', 'sa', 'si'] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n",
      "c:\\Users\\zozom\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:861: UserWarning: unknown class(es) ['Aruba', 'Libyan Arab Jamahiriya', 'Serbia'] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n",
      "Column 'budget': float64\n",
      "Column 'original_language': int32\n",
      "Column 'overview': float64\n",
      "Column 'viewercount': float64\n",
      "Column 'release_date': int32\n",
      "Column 'revenue': float64\n",
      "Column 'runtime': float64\n",
      "Column 'status': int32\n",
      "Column 'vote_count': float64\n",
      "Column 'cast_0_character': int32\n",
      "Column 'cast_0_name': int32\n",
      "Column 'Action': int32\n",
      "Column 'Adventure': int32\n",
      "Column 'Animation': int32\n",
      "Column 'Comedy': int32\n",
      "Column 'Crime': int32\n",
      "Column 'Documentary': int32\n",
      "Column 'Drama': int32\n",
      "Column 'Family': int32\n",
      "Column 'Fantasy': int32\n",
      "Column 'Foreign': int32\n",
      "Column 'History': int32\n",
      "Column 'Horror': int32\n",
      "Column 'Music': int32\n",
      "Column 'Mystery': int32\n",
      "Column 'Romance': int32\n",
      "Column 'Science Fiction': int32\n",
      "Column 'TV Movie': int32\n",
      "Column 'Thriller': int32\n",
      "Column 'War': int32\n",
      "Column 'Western': int32\n",
      "Column 'af': int32\n",
      "Column 'ar': int32\n",
      "Column 'bg': int32\n",
      "Column 'bn': int32\n",
      "Column 'bo': int32\n",
      "Column 'cn': int32\n",
      "Column 'cs': int32\n",
      "Column 'cy': int32\n",
      "Column 'da': int32\n",
      "Column 'de': int32\n",
      "Column 'el': int32\n",
      "Column 'en': int32\n",
      "Column 'eo': int32\n",
      "Column 'es': int32\n",
      "Column 'et': int32\n",
      "Column 'fa': int32\n",
      "Column 'fi': int32\n",
      "Column 'fr': int32\n",
      "Column 'ga': int32\n",
      "Column 'gd': int32\n",
      "Column 'gl': int32\n",
      "Column 'he': int32\n",
      "Column 'hi': int32\n",
      "Column 'hr': int32\n",
      "Column 'hu': int32\n",
      "Column 'hy': int32\n",
      "Column 'is': int32\n",
      "Column 'it': int32\n",
      "Column 'ja': int32\n",
      "Column 'ka': int32\n",
      "Column 'kk': int32\n",
      "Column 'ko': int32\n",
      "Column 'kw': int32\n",
      "Column 'la': int32\n",
      "Column 'ml': int32\n",
      "Column 'mn': int32\n",
      "Column 'nl': int32\n",
      "Column 'no': int32\n",
      "Column 'ny': int32\n",
      "Column 'pa': int32\n",
      "Column 'pl': int32\n",
      "Column 'pt': int32\n",
      "Column 'ro': int32\n",
      "Column 'ru': int32\n",
      "Column 'sh': int32\n",
      "Column 'sk': int32\n",
      "Column 'so': int32\n",
      "Column 'sq': int32\n",
      "Column 'sr': int32\n",
      "Column 'st': int32\n",
      "Column 'sv': int32\n",
      "Column 'sw': int32\n",
      "Column 'ta': int32\n",
      "Column 'te': int32\n",
      "Column 'th': int32\n",
      "Column 'tl': int32\n",
      "Column 'to': int32\n",
      "Column 'tr': int32\n",
      "Column 'uk': int32\n",
      "Column 'ur': int32\n",
      "Column 'vi': int32\n",
      "Column 'xh': int32\n",
      "Column 'xx': int32\n",
      "Column 'yi': int32\n",
      "Column 'zh': int32\n",
      "Column 'zu': int32\n",
      "Column 'Argentina': int32\n",
      "Column 'Australia': int32\n",
      "Column 'Austria': int32\n",
      "Column 'Bahamas': int32\n",
      "Column 'Belgium': int32\n",
      "Column 'Bolivia': int32\n",
      "Column 'Bosnia and Herzegovina': int32\n",
      "Column 'Brazil': int32\n",
      "Column 'Bulgaria': int32\n",
      "Column 'Cambodia': int32\n",
      "Column 'Canada': int32\n",
      "Column 'Chile': int32\n",
      "Column 'China': int32\n",
      "Column 'Czech Republic': int32\n",
      "Column 'Denmark': int32\n",
      "Column 'Dominica': int32\n",
      "Column 'Fiji': int32\n",
      "Column 'Finland': int32\n",
      "Column 'France': int32\n",
      "Column 'Germany': int32\n",
      "Column 'Hong Kong': int32\n",
      "Column 'Hungary': int32\n",
      "Column 'Iceland': int32\n",
      "Column 'India': int32\n",
      "Column 'Ireland': int32\n",
      "Column 'Israel': int32\n",
      "Column 'Italy': int32\n",
      "Column 'Jamaica': int32\n",
      "Column 'Japan': int32\n",
      "Column 'Kazakhstan': int32\n",
      "Column 'Lithuania': int32\n",
      "Column 'Luxembourg': int32\n",
      "Column 'Malaysia': int32\n",
      "Column 'Malta': int32\n",
      "Column 'Mexico': int32\n",
      "Column 'Monaco': int32\n",
      "Column 'Morocco': int32\n",
      "Column 'Netherlands': int32\n",
      "Column 'New Zealand': int32\n",
      "Column 'Norway': int32\n",
      "Column 'Pakistan': int32\n",
      "Column 'Panama': int32\n",
      "Column 'Philippines': int32\n",
      "Column 'Poland': int32\n",
      "Column 'Portugal': int32\n",
      "Column 'Romania': int32\n",
      "Column 'Russia': int32\n",
      "Column 'Singapore': int32\n",
      "Column 'Slovakia': int32\n",
      "Column 'Slovenia': int32\n",
      "Column 'South Africa': int32\n",
      "Column 'South Korea': int32\n",
      "Column 'Spain': int32\n",
      "Column 'Sweden': int32\n",
      "Column 'Switzerland': int32\n",
      "Column 'Taiwan': int32\n",
      "Column 'Thailand': int32\n",
      "Column 'Tunisia': int32\n",
      "Column 'Ukraine': int32\n",
      "Column 'United Arab Emirates': int32\n",
      "Column 'United Kingdom': int32\n",
      "Column 'United States of America': int32\n",
      "Column 'keywords_hash_0': float64\n",
      "Column 'keywords_hash_1': float64\n",
      "Column 'keywords_hash_2': float64\n",
      "Column 'keywords_hash_3': float64\n",
      "Column 'keywords_hash_4': float64\n",
      "Column 'keywords_hash_5': float64\n",
      "Column 'keywords_hash_6': float64\n",
      "Column 'keywords_hash_7': float64\n",
      "Column 'keywords_hash_8': float64\n",
      "Column 'keywords_hash_9': float64\n",
      "Column 'keywords_hash_10': float64\n",
      "Column 'keywords_hash_11': float64\n",
      "Column 'keywords_hash_12': float64\n",
      "Column 'keywords_hash_13': float64\n",
      "Column 'keywords_hash_14': float64\n",
      "Column 'keywords_hash_15': float64\n",
      "Column 'keywords_hash_16': float64\n",
      "Column 'keywords_hash_17': float64\n",
      "Column 'keywords_hash_18': float64\n",
      "Column 'keywords_hash_19': float64\n",
      "Column 'keywords_hash_20': float64\n",
      "Column 'keywords_hash_21': float64\n",
      "Column 'keywords_hash_22': float64\n",
      "Column 'keywords_hash_23': float64\n",
      "Column 'keywords_hash_24': float64\n",
      "Column 'keywords_hash_25': float64\n",
      "Column 'keywords_hash_26': float64\n",
      "Column 'keywords_hash_27': float64\n",
      "Column 'keywords_hash_28': float64\n",
      "Column 'keywords_hash_29': float64\n",
      "Column 'production_companies_hash_0': float64\n",
      "Column 'production_companies_hash_1': float64\n",
      "Column 'production_companies_hash_2': float64\n",
      "Column 'production_companies_hash_3': float64\n",
      "Column 'production_companies_hash_4': float64\n",
      "Column 'production_companies_hash_5': float64\n",
      "Column 'production_companies_hash_6': float64\n",
      "Column 'production_companies_hash_7': float64\n",
      "Column 'production_companies_hash_8': float64\n",
      "Column 'production_companies_hash_9': float64\n",
      "Column 'production_companies_hash_10': float64\n",
      "Column 'production_companies_hash_11': float64\n",
      "Column 'production_companies_hash_12': float64\n",
      "Column 'production_companies_hash_13': float64\n",
      "Column 'production_companies_hash_14': float64\n",
      "Column 'production_companies_hash_15': float64\n",
      "Column 'production_companies_hash_16': float64\n",
      "Column 'production_companies_hash_17': float64\n",
      "Column 'production_companies_hash_18': float64\n",
      "Column 'production_companies_hash_19': float64\n",
      "Polynomial Reggression MSE Testing error: 0.5877663446822016\n",
      "R2:  0.38599048912136336\n",
      "SVR MSE Testing error:  0.6427362601284662\n",
      "R2:  0.3285662231667512\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Test script\n",
    "def test_script(data, movies):\n",
    "\n",
    "    data = pd.read_csv(data)\n",
    "    movies = pd.read_csv(movies)\n",
    "    X = data.iloc[:, :19] \n",
    "    Y = data['vote_average']\n",
    "\n",
    "    X = join_columns(X, movies)\n",
    "\n",
    "    X = nonModels_preprocessing_script(X)\n",
    "    X = apply_encodings(X)\n",
    "\n",
    "    rfe = load_feature_selection()\n",
    "    X = rfe.transform(X)\n",
    "\n",
    "    c = load_model('PolynomialFeatures_model.pkl')\n",
    "    lr = load_model('LinearRegression_model.pkl')\n",
    "    x_test = c.transform(X)\n",
    "    y_pred_test = lr.predict(x_test)\n",
    "    r2 = metrics.r2_score(Y, y_pred_test)\n",
    "    print(\"Polynomial Reggression MSE Testing error: \"+str(mean_squared_error(Y,y_pred_test)))\n",
    "    print('R2: ', r2)\n",
    "\n",
    "    model = load_model('SVM_model.pkl')\n",
    "    Y_pred = model.predict(X)\n",
    "    meanSqErr = mean_squared_error(Y, Y_pred)\n",
    "    r2 = metrics.r2_score(Y, Y_pred)\n",
    "    print('SVR MSE Testing error: ', meanSqErr)\n",
    "    print('R2: ', r2)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "# Test the model using a new CSV file\n",
    "test_script('test.csv', 'movies-credit-students-train.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
