{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# One hot encoding\n",
    "genres_mlb = MultiLabelBinarizer()\n",
    "spoken_languages_mlb = MultiLabelBinarizer()\n",
    "production_countries_mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Label encoding\n",
    "original_language_encoder = LabelEncoder()\n",
    "status_encoder = LabelEncoder()\n",
    "name_encoder = LabelEncoder()\n",
    "character_encoder = LabelEncoder()\n",
    "\n",
    "# Hashing encoding\n",
    "keywords_hash_num = 30    #Change this Number for more accuracy\n",
    "keywords_hash_columns = []\n",
    "hash_column_name = ''\n",
    "\n",
    "for i in range(keywords_hash_num):\n",
    "  hash_column_name = 'keywords_hash_' + str(i)\n",
    "  keywords_hash_columns.append(hash_column_name)\n",
    "\n",
    "keywords_hasher = FeatureHasher(n_features=keywords_hash_num, input_type='string')\n",
    "\n",
    "\n",
    "production_companies_hash_num = 20 #Change this Number for more accuracy\n",
    "production_companies_hash_columns = []\n",
    "hash_column_name = ''\n",
    "\n",
    "for i in range(production_companies_hash_num):\n",
    "  hash_column_name = 'production_companies_hash_' + str(i)\n",
    "  production_companies_hash_columns.append(hash_column_name)\n",
    "\n",
    "production_companies_hasher = FeatureHasher(n_features=production_companies_hash_num, input_type='string')\n",
    "\n",
    "\n",
    "tfidf  = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "\n",
    "cast_num = 1  \n",
    "cast_columns = []\n",
    "cast_character = ''\n",
    "cast_gender = ''\n",
    "cast_name = ''\n",
    "cast_order = ''\n",
    "\n",
    "for i in range(cast_num):\n",
    "  cast_character = 'cast_' + str(i) + '_character'\n",
    "  cast_gender = 'cast_' + str(i) + '_gender'\n",
    "  cast_name = 'cast_' + str(i) + '_name'\n",
    "  cast_order = 'cast_' + str(i) + '_order'\n",
    "\n",
    "  cast_columns.append(cast_character)\n",
    "  cast_columns.append(cast_gender)\n",
    "  cast_columns.append(cast_name)\n",
    "  cast_columns.append(cast_order)\n",
    "\n",
    "rate_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "missingValues = {\n",
    "    'budget'  :  None,\n",
    "    'genres'  :  None,\n",
    "    'homepage'  :  None,\n",
    "    'id'  :  None,\n",
    "    'keywords'  :  None,\n",
    "    'original_language'  :  None,\n",
    "    'original_title'  :  None,\n",
    "    'overview'  :  None,\n",
    "    'viewercount'  :  None,\n",
    "    'production_companies'  :  None,\n",
    "    'production_countries'  :  None,\n",
    "    'release_date'  :  None,\n",
    "    'revenue'  :  None,\n",
    "    'runtime'  :  None,\n",
    "    'spoken_languages'  :  None,\n",
    "    'status'  :  None,\n",
    "    'tagline'  :  None,\n",
    "    'title'  :  None,\n",
    "    'vote_count'  :  None,\n",
    "    'cast'  :  None,\n",
    "    'crew'  :  None,\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the encoding objects\n",
    "def load_encoders():\n",
    "    with open('encoders.pkl', 'rb') as file:\n",
    "        encodings = pickle.load(file)\n",
    "\n",
    "        \n",
    "    genres_mlb = encodings['genres_mlb']\n",
    "    spoken_languages_mlb = encodings['spoken_languages_mlb']\n",
    "    production_countries_mlb = encodings['production_countries_mlb']\n",
    "    original_language_encoder = encodings['original_language_encoder']\n",
    "    status_encoder = encodings['status_encoder']\n",
    "    name_encoder = encodings['name_encoder']\n",
    "    character_encoder = encodings['character_encoder']\n",
    "    rate_encoder = encodings['rate_encoder']\n",
    "    \n",
    "    return genres_mlb, spoken_languages_mlb, production_countries_mlb, original_language_encoder, status_encoder, name_encoder, character_encoder, rate_encoder\n",
    "\n",
    "# Load the feature hashers\n",
    "def load_hashers():\n",
    "    with open('hashers.pkl', 'rb') as file:\n",
    "        hashing = pickle.load(file)\n",
    "\n",
    "    keywords_hasher = hashing['keywords_hasher']\n",
    "    production_companies_hasher = hashing['production_companies_hasher']\n",
    "\n",
    "    return keywords_hasher, production_companies_hasher\n",
    "    \n",
    "\n",
    "# Load the TF-IDF vectorizer\n",
    "def load_tfidf_vectorizer():\n",
    "    with open('tfidf_vectorizer.pkl', 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "# Load the scalers\n",
    "def load_scalers():\n",
    "    with open('scalers.pkl', 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "# Load the feature selection\n",
    "def load_feature_selection():\n",
    "    with open('feature_selection.pkl', 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "# Load the models\n",
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "    \n",
    "# Load the missing values\n",
    "def load_missing_values():\n",
    "    with open('missing_values.pkl', 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a new label in label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unseen(data, encoder, columnName):\n",
    "    unseen_values = list(set(data[columnName]) - set(encoder.classes_))\n",
    "    if unseen_values:\n",
    "        for unseen in unseen_values:\n",
    "            if unseen not in encoder.classes_:\n",
    "                new_label = max(encoder.transform(encoder.classes_)) + 1\n",
    "                encoder.classes_ = np.append(encoder.classes_, unseen)\n",
    "                encoder.transform([unseen])[0] = new_label\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting list of dictionaries to normal list of elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformColumn(enteredData, columnName, dictionaryKey):\n",
    "    enteredData[columnName] = [ast.literal_eval(row) for row in enteredData[columnName]]\n",
    "    for index, row in enteredData[columnName].items():\n",
    "        finalList = []\n",
    "        for j in range(len(row)):\n",
    "            finalList.append(row[j][dictionaryKey])\n",
    "        enteredData.at[index, columnName] = finalList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformMoviesColumns(enteredData, columnName, dictionaryKey1, dictionaryKey2, dictionaryKey3, dictionaryKey4):\n",
    "  enteredData[columnName] = [ast.literal_eval(row) for row in enteredData[columnName]]\n",
    "  for index, row in enteredData[columnName].items():\n",
    "    finalList = []\n",
    "    for j in range(len(row)):\n",
    "      fList = []\n",
    "      fList.append(row[j][dictionaryKey1])\n",
    "      fList.append(row[j][dictionaryKey2])\n",
    "      fList.append(row[j][dictionaryKey3])\n",
    "      fList.append(row[j][dictionaryKey4])\n",
    "      finalList.append(fList)\n",
    "    enteredData.at[index, columnName] = finalList\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fillMissingTestData(data):\n",
    "    missingValues = load_missing_values()\n",
    "    \n",
    "    categoralColumns = ['genres', 'keywords', 'spoken_languages',\n",
    "                        'production_companies', 'production_countries', 'cast', 'crew']\n",
    "    for i in categoralColumns:\n",
    "        data[i] = data[i].apply(lambda x: x if x else missingValues[i])\n",
    "   \n",
    "    numericalColumns = ['budget', 'id', 'viewercount',\n",
    "                    'release_date', 'revenue', 'runtime', 'vote_count']\n",
    "    for i in numericalColumns:\n",
    "        data[i] = data[i].replace(0, missingValues[i])\n",
    "        data[i] =  data[i].fillna(missingValues[i])\n",
    "\n",
    "    textualColumns = ['homepage', 'original_title', 'tagline',\n",
    "                      'title', 'status', 'overview', 'original_language']\n",
    "    for i in textualColumns:\n",
    "        data[i] =  data[i].fillna(missingValues[i])\n",
    "\n",
    "    return data\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the cast and crew columns to X dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_columns(data, movies):\n",
    "  # Add two empty columns to the DataFrame\n",
    "  data = data.join(pd.DataFrame(movies['cast'], columns=['cast'], index = data.index))\n",
    "  data = data.join(pd.DataFrame(movies['crew'], columns=['crew'], index = data.index))\n",
    "\n",
    "  # Put the cast and crew in the right cells\n",
    "  for dataIndex, dataRow in data.iterrows():\n",
    "    if dataRow['id'] in movies['movie_id'].values:\n",
    "      index = movies.loc[movies['movie_id'] == dataRow['id']].index[0]\n",
    "      data.at[dataIndex, 'cast'] = movies.at[index, 'cast']\n",
    "      data.at[dataIndex, 'crew'] = movies.at[index, 'crew']\n",
    "    else:\n",
    "      data.at[dataIndex, 'cast'] = '[]'\n",
    "      data.at[dataIndex, 'crew'] = '[]'\n",
    "  \n",
    "  return data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nonModels_preprocessing_script(data):\n",
    "    # converting list of dectionaries to normal lists of elements using \"transformColumn\" function\n",
    "    # genres column\n",
    "    transformColumn(data, 'genres', 'name')\n",
    "    # keywords column\n",
    "    transformColumn(data, 'keywords', 'name')\n",
    "    # production_companies column\n",
    "    transformColumn(data, 'production_companies', 'name')\n",
    "    # production_countries column\n",
    "    transformColumn(data, 'production_countries', 'name')\n",
    "    # spoken_languages column\n",
    "    transformColumn(data, 'spoken_languages', 'iso_639_1')\n",
    "    # cast column\n",
    "    transformMoviesColumns(data, 'cast', 'character', 'gender', 'name', 'order')\n",
    "    # crew column\n",
    "    transformMoviesColumns(data, 'crew', 'name', 'department', 'gender', 'job')\n",
    "\n",
    "    # Only leaving the year from the release date column\n",
    "    data['release_date'] = data['release_date'].str[-4:].astype(int)\n",
    "\n",
    "    data = fillMissingTestData(data)\n",
    "\n",
    "    # Placing the values from the cast column to a column of it's own  \n",
    "    data = data.join(pd.DataFrame(columns=cast_columns, index = data.index))\n",
    "    for index, row in data.iterrows():\n",
    "      k = 0\n",
    "      for j in range(cast_num):\n",
    "        if(j > len(row['cast']) - 1):\n",
    "          break \n",
    "        data.at[index, cast_columns[k]] = row['cast'][j][0]\n",
    "        k += 1\n",
    "        data.at[index, cast_columns[k]] = row['cast'][j][1]\n",
    "        k += 1\n",
    "        data.at[index, cast_columns[k]] = row['cast'][j][2]\n",
    "        k += 1\n",
    "        data.at[index, cast_columns[k]] = row['cast'][j][3]\n",
    "        k += 1\n",
    "\n",
    "    data.drop('cast',axis=1,inplace=True)\n",
    "    \n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing test Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the loaded encodings to the test data\n",
    "def apply_encodings(data, y):\n",
    "    # Load the encoding objects, hashers, vectorizer, and scalers\n",
    "    genres_mlb, spoken_languages_mlb, production_countries_mlb, original_language_encoder, status_encoder, name_encoder, character_encoder, rate_encoder =  load_encoders()\n",
    "    keywords_hasher, production_companies_hasher =  load_hashers()\n",
    "    tfidf = load_tfidf_vectorizer()\n",
    "    scaler = load_scalers()\n",
    "    \n",
    "    # Apply one-hot encoding\n",
    "    # genres column\n",
    "    data = data.join(pd.DataFrame(genres_mlb.transform(data.pop('genres')),\n",
    "                                    columns=genres_mlb.classes_,\n",
    "                                    index=data.index))\n",
    "    \n",
    "    # spoken_languages column\n",
    "    data = data.join(pd.DataFrame(spoken_languages_mlb.transform(data.pop('spoken_languages')),\n",
    "                                    columns=spoken_languages_mlb.classes_,\n",
    "                                    index=data.index))\n",
    "    # production_countries column \n",
    "    data = data.join(pd.DataFrame(production_countries_mlb.transform(data.pop('production_countries')),\n",
    "                                    columns=production_countries_mlb.classes_,\n",
    "                                    index=data.index))\n",
    "    \n",
    "\n",
    "    # Apply lable encoding\n",
    "    # original_language column\n",
    "    unseen(data, original_language_encoder, 'original_language')\n",
    "    data['original_language'] = original_language_encoder.transform(data['original_language'])\n",
    "    # status column\n",
    "    unseen(data, status_encoder, 'status')\n",
    "    data['status'] = status_encoder.transform(data['status'])\n",
    "    # cast column\n",
    "    for i in range(cast_num):\n",
    "        cast_name = 'cast_' + str(i) + '_name'\n",
    "        \n",
    "        unseen(data, name_encoder, cast_name)\n",
    "        data[cast_name] = name_encoder.transform(data[cast_name])\n",
    "\n",
    "\n",
    "        cast_character = 'cast_' + str(i) + '_character'\n",
    "\n",
    "        unseen(data, character_encoder, cast_character)\n",
    "        data[cast_character] = character_encoder.transform(data[cast_character])\n",
    "            \n",
    "    # Apply hashing encoding\n",
    "    # keywords column\n",
    "    data = data.join(pd.DataFrame((keywords_hasher.transform(data.pop('keywords')).toarray()), columns=keywords_hash_columns, index=data.index))\n",
    "    # production_companies column\n",
    "    data = data.join(pd.DataFrame((production_companies_hasher.transform(data.pop('production_companies')).toarray()), columns=production_companies_hash_columns, index=data.index))\n",
    "\n",
    "\n",
    "    # Apply TF-IDF \n",
    "    overview_vectors  = tfidf.transform(data['overview'])        \n",
    "    data['overview'] = list(overview_vectors.toarray())\n",
    "    data['overview'] = data['overview'].apply(lambda x: sum(x) / len(x))\n",
    "\n",
    "\n",
    "    # Normalizing the numerical columns\n",
    "    num_cols = data.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "    num_cols.append(\"vote_count\")\n",
    "    # print(len(num_cols))\n",
    "\n",
    "    # for column in num_cols:\n",
    "    #     print(f\"Column '{column}': {data[column].dtype}\")\n",
    "    \n",
    "\n",
    "    data[num_cols] = scaler.transform(data[num_cols])\n",
    "\n",
    "\n",
    "        \n",
    "    data.drop('homepage',axis=1,inplace=True)\n",
    "    data.drop('id',axis=1,inplace=True)\n",
    "    data.drop('original_title',axis=1,inplace=True)\n",
    "    data.drop('tagline',axis=1,inplace=True)\n",
    "    data.drop('title',axis=1,inplace=True)\n",
    "    data.drop('crew',axis=1,inplace=True)\n",
    "\n",
    "    y = rate_encoder.transform(y)\n",
    "\n",
    "    return data, y\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zozom\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:861: UserWarning: unknown class(es) ['ku', 'mi', 'ne', 'nv', 'ps'] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n",
      "c:\\Users\\zozom\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:861: UserWarning: unknown class(es) ['Cyprus', 'Greece', 'Jordan', 'Nigeria', 'Peru', 'Turkey'] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier accuracy: 0.28%\n",
      "Random Forest Classifier accuracy: 0.53%\n",
      "Logistic Regression Classifier accuracy: 0.27%\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Test script\n",
    "def test_script(data, movies):\n",
    "\n",
    "    data = pd.read_csv(data)\n",
    "    X = data.iloc[:, :19] \n",
    "    Y = data['Rate']\n",
    "    movies = pd.read_csv(movies)\n",
    "\n",
    "    X = join_columns(X, movies)\n",
    "\n",
    "    X = nonModels_preprocessing_script(X)\n",
    "    X, Y = apply_encodings(X, Y)\n",
    "\n",
    "    rfe = load_feature_selection()\n",
    "    X = rfe.transform(X)\n",
    "\n",
    "    best_dt = load_model('best_dt_model.pkl')\n",
    "    y_pred = best_dt.predict(X)\n",
    "    best_dt_accuracy = accuracy_score(Y, y_pred)\n",
    "    print(f\"Decision Tree Classifier accuracy: {best_dt_accuracy:.2f}%\")\n",
    "\n",
    "    best_rf  = load_model('best_rf_model.pkl')\n",
    "    y_pred = best_rf.predict(X)\n",
    "    best_rf_accuracy = accuracy_score(Y, y_pred)\n",
    "    print(f\"Random Forest Classifier accuracy: {best_rf_accuracy:.2f}%\")\n",
    "\n",
    "    best_lr  = load_model('best_lr_model.pkl')\n",
    "    y_pred = best_lr.predict(X)\n",
    "    best_lr_accuracy = accuracy_score(Y, y_pred)\n",
    "    print(f\"Logistic Regression Classifier accuracy: {best_lr_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "# Test the model using a new CSV file\n",
    "test_script('movies-tas-test.csv', 'credit-tas-test.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
